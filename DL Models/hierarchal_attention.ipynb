{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANModule(keras.layers.Layer):\n",
    "    def __init__(self, num_cells, **kwargs):\n",
    "        self.num_cells = num_cells\n",
    "        super(HANModule, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"W\",\n",
    "                                 shape=(self.num_cells * 2, 1),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True)\n",
    "        super(HANModule, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(HANModule, self).get_config()\n",
    "        config[\"num_cells\"] = self.num_cells\n",
    "        return config\n",
    "\n",
    "    def call(self, x):\n",
    "        return keras.backend.dot(x, self.W)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_length, num_sentences, vocab_size, embedding_size, num_cells, num_output, dropout_prob=0.2):\n",
    "    input_layer = keras.layers.Input(shape=(num_sentences, input_length,))\n",
    "    input_sentences = keras.layers.Lambda(lambda x: tf.unstack(x, axis=1))(input_layer)\n",
    "    embedding_layer = keras.layers.Embedding(vocab_size + 1, embedding_size)\n",
    "    out = []\n",
    "    sentence_gru_layer = keras.layers.Bidirectional(keras.layers.GRU(num_cells, return_sequences=True))\n",
    "    sentence_dense_layer = keras.layers.TimeDistributed(keras.layers.Dense(num_cells * 2, activation=\"tanh\"))\n",
    "    sentence_embedding_dropout_layer = keras.layers.Dropout(dropout_prob)\n",
    "    sentence_gru_dropout_layer = keras.layers.Dropout(dropout_prob)\n",
    "    sentence_dense_dropout_layer = keras.layers.Dropout(dropout_prob)\n",
    "    sentence_han_layer = HANModule(num_cells)\n",
    "    for i in range(len(input_sentences)):\n",
    "        embeddings = embedding_layer(input_sentences[i])\n",
    "        embeddings = sentence_embedding_dropout_layer(embeddings)\n",
    "        gru_out = sentence_gru_layer(embeddings)\n",
    "        gru_out = sentence_gru_dropout_layer(gru_out)\n",
    "        dense_out = sentence_dense_layer(gru_out)\n",
    "        dense_out = sentence_dense_dropout_layer(dense_out)\n",
    "        attention_out = sentence_han_layer(dense_out)\n",
    "        attention_out = keras.layers.Flatten()(attention_out)\n",
    "        softmax_out = keras.layers.Activation(\"softmax\", name=\"attention_word_{0}\".format(str(i)))(attention_out)\n",
    "        softmax_out = keras.layers.Reshape((input_length, 1))(softmax_out)\n",
    "        mul_out = keras.layers.Multiply()([softmax_out, gru_out])\n",
    "        sum_out = keras.layers.Lambda(lambda x: keras.backend.sum(x, axis=1))(mul_out)\n",
    "        sum_out = keras.layers.Flatten()(sum_out)\n",
    "        out.append(sum_out)\n",
    "    stacked_input = keras.layers.Lambda(lambda x: tf.stack(x, axis=1))(out)\n",
    "    gru_out = keras.layers.Bidirectional(keras.layers.GRU(num_cells, return_sequences=True))(stacked_input)\n",
    "    gru_out = keras.layers.Dropout(dropout_prob)(gru_out)\n",
    "    dense_out = keras.layers.TimeDistributed(keras.layers.Dense(num_cells * 2, activation=\"tanh\"))(gru_out)\n",
    "    dense_out = keras.layers.Dropout(dropout_prob)(dense_out)\n",
    "    attention_out = HANModule(num_cells)(dense_out)\n",
    "    attention_out = keras.layers.Flatten()(attention_out)\n",
    "    softmax_out = keras.layers.Activation(\"softmax\", name=\"attention_sentence\")(attention_out)\n",
    "    softmax_out = keras.layers.Reshape((num_sentences, 1))(softmax_out)\n",
    "    mul_out = keras.layers.Multiply()([softmax_out, gru_out])\n",
    "    sum_out = keras.layers.Lambda(lambda x: keras.backend.sum(x, axis=1))(mul_out)\n",
    "    out = keras.layers.Dense(num_output, activation=\"softmax\")(sum_out)\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_location):\n",
    "    return keras.models.load_model(model_location, custom_objects={\"tf\": tf, \"HANModule\": HANModule, \"keras\": keras})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputs(input_data, sequence_length, num_sentences, sentence_sep=\".\",\n",
    "                   pad_direction=\"pre\", truncate_direction=\"pre\", tokenizer=None):\n",
    "    assert pad_direction in [\"pre\", \"post\"]\n",
    "    assert truncate_direction in [\"pre\", \"post\"]\n",
    "    if not tokenizer:\n",
    "        tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(input_data)\n",
    "    x = np.zeros((len(input_data), num_sentences, sequence_length), dtype=\"int32\")\n",
    "\n",
    "    for i in range(len(input_data)):\n",
    "        sentences = input_data[i].split(sentence_sep)\n",
    "        if sentences[-1] == \"\":\n",
    "            sentences = sentences[:-1]\n",
    "        sentences_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "        padded_sentences = keras.preprocessing.sequence.pad_sequences(sentences_sequences,\n",
    "                                                                      sequence_length,\n",
    "                                                                      padding=pad_direction,\n",
    "                                                                      truncating=truncate_direction)\n",
    "        if truncate_direction == \"pre\":\n",
    "            padded_sentences = padded_sentences[-num_sentences:]\n",
    "        else:\n",
    "            padded_sentences = padded_sentences[:num_sentences]\n",
    "\n",
    "        if pad_direction == \"pre\":\n",
    "            x[i, -len(padded_sentences):] = padded_sentences\n",
    "        else:\n",
    "            x[i, :len(padded_sentences)] = padded_sentences\n",
    "    return x, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(output_data):\n",
    "    output_distribution = output_data.value_counts()\n",
    "    output_classes = output_distribution.index\n",
    "    max_count = max(output_distribution)\n",
    "    output_distribution = list(max_count / output_distribution)\n",
    "    class_weights = {i: output_distribution[i] for i in range(len(output_distribution))}\n",
    "    output_mapping = {output_classes[i]: i for i in range(len(output_classes))}\n",
    "    y = np.array([output_mapping[i] for i in output_data])\n",
    "    y = keras.utils.to_categorical(y, num_classes=len(output_mapping))\n",
    "    return y, output_mapping, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_file_location, input_col_name, output_col_name,\n",
    "          embedding_length, pad_direction, truncate_direction, sequence_length, num_sentence, sentence_sep, num_cells, epochs, batch_size):\n",
    "    data = pd.read_csv(data_file_location).sample(frac=1.0).reset_index(drop=True).iloc[:20000].dropna()\n",
    "    input_sentences = data[input_col_name].values.tolist()\n",
    "    output_classes = data[output_col_name]\n",
    "    x, tokenizer = process_inputs(input_sentences, sequence_length, num_sentence, sentence_sep, pad_direction,\n",
    "                                  truncate_direction)\n",
    "    y, output_mapping, class_weights = process_output(output_classes)\n",
    "    model_metadata = {\"tokenizer\": tokenizer, \"output_mapping\": output_mapping, \"sentence_sep\": sentence_sep, \"pad_direction\": pad_direction,\n",
    "                      \"truncate_direction\": truncate_direction, \"sequence_length\": sequence_length, \"num_sentence\": num_sentence}\n",
    "    pickle.dump(model_metadata, open(\"model_artifacts/model_metadata.pkl\", \"wb+\"))\n",
    "    model = get_model(sequence_length, num_sentence, len(tokenizer.word_index), embedding_length, num_cells,\n",
    "                      len(output_mapping))\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        \"model_artifacts/weights.{epoch:02d}.hdf5\")\n",
    "    model.fit(x, y, validation_split=0.1, epochs=epochs, class_weight=class_weights, batch_size=batch_size, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sentences, model, model_metadata):\n",
    "    tokenizer, output_mapping, sentence_sep, pad_direction, \\\n",
    "            truncate_direction, sequence_length, num_sentences = model_metadata[\"tokenizer\"], model_metadata[\"output_mapping\"], \\\n",
    "                                                                 model_metadata[\"sentence_sep\"], model_metadata[\"pad_direction\"], \\\n",
    "                                                                 model_metadata[\"truncate_direction\"], \\\n",
    "                                                                 model_metadata[\"sequence_length\"], model_metadata[\"num_sentence\"]\n",
    "    x = process_inputs(input_sentences, sequence_length, num_sentences, sentence_sep, pad_direction,\n",
    "                       truncate_direction, tokenizer)\n",
    "    model_output = model.predict(x)\n",
    "    output_mapping = {j: i for i, j in output_mapping.items()}\n",
    "    argsort_output = np.argsort(model_output)\n",
    "    return [[output_mapping[j] for j in i[::-1]] for i in argsort_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(input_sentence, model, model_metadata):\n",
    "    tokenizer, output_mapping, sentence_sep, pad_direction, \\\n",
    "            truncate_direction, sequence_length, num_sentences = model_metadata[\"tokenizer\"], model_metadata[\"output_mapping\"], \\\n",
    "                                                                 model_metadata[\"sentence_sep\"], model_metadata[\"pad_direction\"], \\\n",
    "                                                                 model_metadata[\"truncate_direction\"], \\\n",
    "                                                                 model_metadata[\"sequence_length\"], model_metadata[\"num_sentence\"]\n",
    "    x = process_inputs([input_sentence], sequence_length, num_sentences, sentence_sep, pad_direction,\n",
    "                       truncate_direction, tokenizer)\n",
    "    sentence_attention = keras.models.Model(inputs = model.inputs, outputs=model.get_layer(\"attention_sentence\").output).predict(x[0])[0]\n",
    "    rev_word_index = {j: i for i, j in tokenizer.word_index.items()}\n",
    "    rev_word_index[0] = \"<blank>\"\n",
    "    for i in range(num_sentences):\n",
    "        word_attention = keras.models.Model(inputs = model.inputs, outputs=model.get_layer(\"attention_word_{0}\".format(str(i))).output).predict(x[0])[0]\n",
    "        colorstr = \"\"\n",
    "        for j in range(len(word_attention)):\n",
    "            colorstr += \"<span style='font-size:{0}%;'>{1} </span>\".format(100 + 4 * (word_attention[j] * 100), rev_word_index[x[0][0][i][j]])\n",
    "        colorstr = \"<span style='color:hsl(360, 100%, {0}%)'>\".format((100 - (sentence_attention[i] * 50) - 15)) + colorstr + \"</span>\"\n",
    "        display(Markdown(colorstr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size:100%;'>himanshu</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorstr = \"<span style='font-size:100%;'>himanshu</span>\"\n",
    "display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 17858 samples, validate on 1985 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "17858/17858 [==============================] - 167s 9ms/sample - loss: 1.5659 - acc: 0.4469 - val_loss: 1.4139 - val_acc: 0.5411\n",
      "Epoch 2/5\n",
      "17858/17858 [==============================] - 151s 8ms/sample - loss: 1.2567 - acc: 0.5279 - val_loss: 1.2739 - val_acc: 0.5194\n",
      "Epoch 3/5\n",
      "17858/17858 [==============================] - 158s 9ms/sample - loss: 1.0040 - acc: 0.6154 - val_loss: 1.3681 - val_acc: 0.5224\n",
      "Epoch 4/5\n",
      "17858/17858 [==============================] - 177s 10ms/sample - loss: 0.7844 - acc: 0.6762 - val_loss: 1.5874 - val_acc: 0.5098\n",
      "Epoch 5/5\n",
      "17858/17858 [==============================] - 157s 9ms/sample - loss: 0.6127 - acc: 0.7240 - val_loss: 1.9043 - val_acc: 0.4942\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 20\n",
    "num_sentence = 10\n",
    "embedding_length = 100\n",
    "data_file_location = \"E:\\Electronics_5_sample.csv\"\n",
    "input_col_name = \"reviewText\"\n",
    "output_col_name = \"overall\"\n",
    "pad_direction = \"pre\"\n",
    "truncate_direction = \"pre\"\n",
    "sentence_sep = \".\"\n",
    "num_cells = 50\n",
    "epochs = 5\n",
    "batch_size = 200\n",
    "train(data_file_location, input_col_name, output_col_name, embedding_length,\n",
    "      pad_direction, truncate_direction, sequence_length, num_sentence,\n",
    "      sentence_sep, num_cells, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata = pickle.load(open(\"model_artifacts/model_metadata.pkl\", \"rb\"))\n",
    "model = load_model(\"model_artifacts/weights.01.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 78.4986263513565%)'><span style='font-size:104.7205463051796%;'><blank> </span><span style='font-size:104.63782288134098%;'><blank> </span><span style='font-size:104.82945702970028%;'><blank> </span><span style='font-size:105.22045269608498%;'><blank> </span><span style='font-size:105.7732306420803%;'><blank> </span><span style='font-size:106.47165700793266%;'><blank> </span><span style='font-size:107.31459781527519%;'><blank> </span><span style='font-size:108.31368565559387%;'><blank> </span><span style='font-size:109.49304923415184%;'><blank> </span><span style='font-size:110.89034453034401%;'><blank> </span><span style='font-size:112.55918890237808%;'><blank> </span><span style='font-size:114.57344740629196%;'><blank> </span><span style='font-size:117.0345813035965%;'><blank> </span><span style='font-size:120.08417844772339%;'><blank> </span><span style='font-size:123.92605543136597%;'><blank> </span><span style='font-size:128.86740863323212%;'><blank> </span><span style='font-size:135.40052771568298%;'><blank> </span><span style='font-size:144.3786382675171%;'><blank> </span><span style='font-size:157.43206143379211%;'><blank> </span><span style='font-size:178.07905673980713%;'><blank> </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 80.47310099005699%)'><span style='font-size:104.7205463051796%;'><blank> </span><span style='font-size:104.63782288134098%;'><blank> </span><span style='font-size:104.82945702970028%;'><blank> </span><span style='font-size:105.22045269608498%;'><blank> </span><span style='font-size:105.7732306420803%;'><blank> </span><span style='font-size:106.47165700793266%;'><blank> </span><span style='font-size:107.31459781527519%;'><blank> </span><span style='font-size:108.31368565559387%;'><blank> </span><span style='font-size:109.49304923415184%;'><blank> </span><span style='font-size:110.89034453034401%;'><blank> </span><span style='font-size:112.55918890237808%;'><blank> </span><span style='font-size:114.57344740629196%;'><blank> </span><span style='font-size:117.0345813035965%;'><blank> </span><span style='font-size:120.08417844772339%;'><blank> </span><span style='font-size:123.92605543136597%;'><blank> </span><span style='font-size:128.86740863323212%;'><blank> </span><span style='font-size:135.40052771568298%;'><blank> </span><span style='font-size:144.3786382675171%;'><blank> </span><span style='font-size:157.43206143379211%;'><blank> </span><span style='font-size:178.07905673980713%;'><blank> </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 81.38334661722183%)'><span style='font-size:104.7205463051796%;'><blank> </span><span style='font-size:104.63782288134098%;'><blank> </span><span style='font-size:104.82945702970028%;'><blank> </span><span style='font-size:105.22045269608498%;'><blank> </span><span style='font-size:105.7732306420803%;'><blank> </span><span style='font-size:106.47165700793266%;'><blank> </span><span style='font-size:107.31459781527519%;'><blank> </span><span style='font-size:108.31368565559387%;'><blank> </span><span style='font-size:109.49304923415184%;'><blank> </span><span style='font-size:110.89034453034401%;'><blank> </span><span style='font-size:112.55918890237808%;'><blank> </span><span style='font-size:114.57344740629196%;'><blank> </span><span style='font-size:117.0345813035965%;'><blank> </span><span style='font-size:120.08417844772339%;'><blank> </span><span style='font-size:123.92605543136597%;'><blank> </span><span style='font-size:128.86740863323212%;'><blank> </span><span style='font-size:135.40052771568298%;'><blank> </span><span style='font-size:144.3786382675171%;'><blank> </span><span style='font-size:157.43206143379211%;'><blank> </span><span style='font-size:178.07905673980713%;'><blank> </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 81.94209042936563%)'><span style='font-size:104.7205463051796%;'><blank> </span><span style='font-size:104.63782288134098%;'><blank> </span><span style='font-size:104.82945702970028%;'><blank> </span><span style='font-size:105.22045269608498%;'><blank> </span><span style='font-size:105.7732306420803%;'><blank> </span><span style='font-size:106.47165700793266%;'><blank> </span><span style='font-size:107.31459781527519%;'><blank> </span><span style='font-size:108.31368565559387%;'><blank> </span><span style='font-size:109.49304923415184%;'><blank> </span><span style='font-size:110.89034453034401%;'><blank> </span><span style='font-size:112.55918890237808%;'><blank> </span><span style='font-size:114.57344740629196%;'><blank> </span><span style='font-size:117.0345813035965%;'><blank> </span><span style='font-size:120.08417844772339%;'><blank> </span><span style='font-size:123.92605543136597%;'><blank> </span><span style='font-size:128.86740863323212%;'><blank> </span><span style='font-size:135.40052771568298%;'><blank> </span><span style='font-size:144.3786382675171%;'><blank> </span><span style='font-size:157.43206143379211%;'><blank> </span><span style='font-size:178.07905673980713%;'><blank> </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 82.42542710155249%)'><span style='font-size:104.7205463051796%;'><blank> </span><span style='font-size:104.63782288134098%;'><blank> </span><span style='font-size:104.82945702970028%;'><blank> </span><span style='font-size:105.22045269608498%;'><blank> </span><span style='font-size:105.7732306420803%;'><blank> </span><span style='font-size:106.47165700793266%;'><blank> </span><span style='font-size:107.31459781527519%;'><blank> </span><span style='font-size:108.31368565559387%;'><blank> </span><span style='font-size:109.49304923415184%;'><blank> </span><span style='font-size:110.89034453034401%;'><blank> </span><span style='font-size:112.55918890237808%;'><blank> </span><span style='font-size:114.57344740629196%;'><blank> </span><span style='font-size:117.0345813035965%;'><blank> </span><span style='font-size:120.08417844772339%;'><blank> </span><span style='font-size:123.92605543136597%;'><blank> </span><span style='font-size:128.86740863323212%;'><blank> </span><span style='font-size:135.40052771568298%;'><blank> </span><span style='font-size:144.3786382675171%;'><blank> </span><span style='font-size:157.43206143379211%;'><blank> </span><span style='font-size:178.07905673980713%;'><blank> </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 83.78017092123628%)'><span style='font-size:111.9294598698616%;'><blank> </span><span style='font-size:111.90865263342857%;'><blank> </span><span style='font-size:112.67248839139938%;'><blank> </span><span style='font-size:114.12331163883209%;'><blank> </span><span style='font-size:116.33408814668655%;'><blank> </span><span style='font-size:119.60434168577194%;'><blank> </span><span style='font-size:131.59812092781067%;'>i </span><span style='font-size:136.33569180965424%;'>bought </span><span style='font-size:131.26491904258728%;'>this </span><span style='font-size:109.70601662993431%;'>lens </span><span style='font-size:146.50352895259857%;'>because </span><span style='font-size:125.86284577846527%;'>lately </span><span style='font-size:114.1014963388443%;'>i've </span><span style='font-size:114.35286849737167%;'>been </span><span style='font-size:118.17687004804611%;'>sort </span><span style='font-size:117.5527423620224%;'>of </span><span style='font-size:126.97702050209045%;'>in </span><span style='font-size:104.15419787168503%;'>love </span><span style='font-size:116.7123094201088%;'>with </span><span style='font-size:120.12903392314911%;'>lenses </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 83.67025669664145%)'><span style='font-size:112.06335201859474%;'>i </span><span style='font-size:105.01824989914894%;'>travel </span><span style='font-size:110.46695038676262%;'>a </span><span style='font-size:104.99511882662773%;'>lot </span><span style='font-size:103.52796614170074%;'>and </span><span style='font-size:106.57555684447289%;'>i </span><span style='font-size:106.5852902829647%;'>like </span><span style='font-size:110.94803214073181%;'>visiting </span><span style='font-size:116.63995832204819%;'>a </span><span style='font-size:108.51939022541046%;'>lot </span><span style='font-size:112.74892836809158%;'>of </span><span style='font-size:109.78233367204666%;'>spots </span><span style='font-size:114.30900543928146%;'>most </span><span style='font-size:122.01040834188461%;'>of </span><span style='font-size:143.8704788684845%;'>them </span><span style='font-size:131.27542436122894%;'>being </span><span style='font-size:136.34455502033234%;'>buildings </span><span style='font-size:141.15647077560425%;'>with </span><span style='font-size:139.2597794532776%;'>history </span><span style='font-size:163.90274167060852%;'>etc </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 84.14724687114358%)'><span style='font-size:114.69804495573044%;'><blank> </span><span style='font-size:113.96778374910355%;'><blank> </span><span style='font-size:114.41269516944885%;'><blank> </span><span style='font-size:126.57857835292816%;'>the </span><span style='font-size:103.51582989096642%;'>great </span><span style='font-size:123.94513785839081%;'>thing </span><span style='font-size:114.603191614151%;'>is </span><span style='font-size:125.72813928127289%;'>that </span><span style='font-size:130.6143969297409%;'>the </span><span style='font-size:110.13194993138313%;'>lens </span><span style='font-size:114.34132158756256%;'>is </span><span style='font-size:110.30609533190727%;'>very </span><span style='font-size:114.49169665575027%;'>wide </span><span style='font-size:114.37739878892899%;'>angled </span><span style='font-size:109.56038013100624%;'>and </span><span style='font-size:115.13583958148956%;'>has </span><span style='font-size:106.16682022809982%;'>amazing </span><span style='font-size:137.27712035179138%;'>low </span><span style='font-size:156.61287903785706%;'>light </span><span style='font-size:143.53472292423248%;'>performance </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 67.3703184723854%)'><span style='font-size:101.2062975205481%;'><blank> </span><span style='font-size:101.27057135105133%;'><blank> </span><span style='font-size:101.42805622890592%;'><blank> </span><span style='font-size:101.67658906430006%;'><blank> </span><span style='font-size:102.02608294785023%;'><blank> </span><span style='font-size:102.4982389062643%;'><blank> </span><span style='font-size:103.13079915940762%;'><blank> </span><span style='font-size:103.98804657161236%;'><blank> </span><span style='font-size:105.18353693187237%;'><blank> </span><span style='font-size:106.93128854036331%;'><blank> </span><span style='font-size:109.67172607779503%;'><blank> </span><span style='font-size:118.67625266313553%;'>i </span><span style='font-size:138.99564445018768%;'>do </span><span style='font-size:291.40182733535767%;'>not </span><span style='font-size:133.835107088089%;'>regret </span><span style='font-size:125.23716688156128%;'>one </span><span style='font-size:113.01716268062592%;'>bit </span><span style='font-size:114.98797535896301%;'>paying </span><span style='font-size:112.52227574586868%;'>for </span><span style='font-size:112.3153530061245%;'>this </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:hsl(360, 100%, 76.30942150950432%)'><span style='font-size:136.3522708415985%;'>try </span><span style='font-size:134.56985354423523%;'>it </span><span style='font-size:118.90083402395248%;'>on </span><span style='font-size:105.55360727012157%;'>macro </span><span style='font-size:105.56655079126358%;'>related </span><span style='font-size:101.52188427746296%;'>photography </span><span style='font-size:103.60101945698261%;'>nor </span><span style='font-size:103.43937873840332%;'>have </span><span style='font-size:105.32187595963478%;'>i </span><span style='font-size:117.23226606845856%;'>tried </span><span style='font-size:102.85878404974937%;'>portraits </span><span style='font-size:107.6732188463211%;'>for </span><span style='font-size:110.98448187112808%;'>which </span><span style='font-size:118.98982673883438%;'>i'm </span><span style='font-size:131.06902539730072%;'>sure </span><span style='font-size:154.63183522224426%;'>it </span><span style='font-size:185.6705605983734%;'>would </span><span style='font-size:133.95880162715912%;'>shine </span><span style='font-size:114.72829282283783%;'>very </span><span style='font-size:107.37561881542206%;'>well </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  5  >  4  >  3  >  2  >  1\n"
     ]
    }
   ],
   "source": [
    "input_sentences = [\"Apple shutdown its office. China person dies\"]\n",
    "data = pd.read_csv(data_file_location).sample(frac=1.0).reset_index(drop=True).iloc[:10].dropna()\n",
    "input_sentences = data[input_col_name].values.tolist()\n",
    "visualize_attention(model=model, model_metadata=model_metadata, input_sentence=input_sentences[0])\n",
    "print(\"Prediction: \", \"  >  \".join([str(i) for i in predict(input_sentences[0], model, model_metadata)[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-c68234464b25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "[i for i in a if i != [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
