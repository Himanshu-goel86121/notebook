{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = \"E:/datasets/babi/tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt\"\n",
    "maxlen = 10\n",
    "num_sentences = 60\n",
    "embedding_size=200\n",
    "num_episodes = 1\n",
    "num_cells = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_location) as f:\n",
    "    data = f.read().split(\"\\n\")\n",
    "sep_data = []\n",
    "temp = []\n",
    "for i in range(len(data) - 1):\n",
    "    if data[i + 1].split(\" \")[0] == \"1\":\n",
    "        temp.append(\" \".join(data[i].split(\" \")[1: ]))\n",
    "        sep_data.append(temp)\n",
    "        temp = []\n",
    "    else:\n",
    "        temp.append(\" \".join(data[i].split(\" \")[1:]))\n",
    "sep_data.append(temp)\n",
    "input_sentences = []\n",
    "questions = []\n",
    "answers = []\n",
    "for i in range(len(sep_data)):\n",
    "    temp = []\n",
    "    for j in range(len(sep_data[i])):\n",
    "        sen_tab_split = sep_data[i][j].split(\"\\t\")\n",
    "        if len(sen_tab_split) == 3:\n",
    "            input_sentences.append(temp.copy())\n",
    "            questions.append(sen_tab_split[0])\n",
    "            answers.append(sen_tab_split[1])\n",
    "        else:\n",
    "            temp.append(sep_data[i][j])\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "for i in range(len(input_sentences)):\n",
    "    tokenizer.fit_on_texts(input_sentences[i])\n",
    "tokenizer.fit_on_texts(questions)\n",
    "for i in range(len(input_sentences)):\n",
    "    input_sentences[i] = tokenizer.texts_to_sequences(input_sentences[i])\n",
    "    input_sentences[i] = keras.preprocessing.sequence.pad_sequences(input_sentences[i], maxlen=maxlen)\n",
    "questions = tokenizer.texts_to_sequences(questions)\n",
    "questions = keras.preprocessing.sequence.pad_sequences(questions, maxlen=maxlen)\n",
    "zeros = np.zeros((num_sentences, maxlen)).tolist()\n",
    "for i in range(len(input_sentences)):\n",
    "    input_sentences[i] = zeros + input_sentences[i].tolist()\n",
    "    input_sentences[i] = input_sentences[i][-num_sentences:]\n",
    "input_sentences = np.array(input_sentences)\n",
    "answers = np.array([tokenizer.word_index[i] - 1 for i in answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(keras.layers.Layer):\n",
    "    def __init__(self, input_length, embedding_size, **kwargs):\n",
    "        self.input_length = input_length\n",
    "        self.embedding_size = embedding_size\n",
    "        super(PositionalEncoder, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(PositionalEncoder, self).build(input_shape)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoder, self).get_config()\n",
    "        config[\"input_length\"] = self.input_length\n",
    "        config[\"embedding_size\"] = self.embedding_size\n",
    "        return config\n",
    "\n",
    "    def call(self, x):\n",
    "        l = []\n",
    "        for i in range(self.input_length):\n",
    "            l.append((1 - (i / self.input_length)) + (1 / self.embedding_size) * (1 - (2 * (i / self.input_length))))\n",
    "        l = [l for i in range(keras.backend.int_shape(x)[1])]\n",
    "        l = np.array(l)\n",
    "        l = np.reshape(l, (keras.backend.int_shape(x)[1], self.input_length, 1))\n",
    "        temp = x * l\n",
    "        out = keras.layers.Lambda(lambda x: keras.backend.sum(x, axis=2))(temp)\n",
    "        return out\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (keras.backend.int_shape(input_shape)[1], keras.backend.int_shape(input_shape)[3],)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_length, vocab_size, embedding_size, episodes, num_sentences=10, num_cells=128):\n",
    "    input_sentence = keras.layers.Input(shape=(num_sentences, input_length,))\n",
    "    question_sentence = keras.layers.Input(shape=(input_length,))\n",
    "    embeddings = keras.layers.Embedding(vocab_size + 1, embedding_size)\n",
    "    input_sentence_emb = embeddings(input_sentence)\n",
    "    question_sentence_emb = embeddings(question_sentence)\n",
    "    \n",
    "    pe_features = PositionalEncoder(input_length, embedding_size)(input_sentence_emb)\n",
    "    gru_features_input = keras.layers.Bidirectional(keras.layers.GRU(num_cells, return_sequences=True))(pe_features)\n",
    "    gru_features_question = keras.layers.Bidirectional(keras.layers.GRU(num_cells))(question_sentence_emb)\n",
    "    m = gru_features_question\n",
    "    attention_dense1_layer = keras.layers.TimeDistributed(keras.layers.Dense(num_cells * 2, activation=\"tanh\"))\n",
    "    attention_dense2_layer = keras.layers.TimeDistributed(keras.layers.Dense(1))\n",
    "    rt_kernel_layer = keras.layers.Dense(num_cells * 2, activation=None, use_bias=False)\n",
    "    rt_recurrent_layer = keras.layers.Dense(num_cells * 2, activation=None)\n",
    "    h_bar_kernel_layer = keras.layers.Dense(num_cells * 2, activation=None)\n",
    "    h_bar_recurrent_layer = keras.layers.Dense(num_cells * 2, activation=None, use_bias=False)\n",
    "    for _ in range(episodes):\n",
    "        ct_mul_q = keras.layers.Multiply()([gru_features_input, gru_features_question])\n",
    "        ct_mul_m = keras.layers.Multiply()([gru_features_input, m])\n",
    "        ct_sub_q = keras.layers.Subtract()([gru_features_input, gru_features_question])\n",
    "        ct_sub_m = keras.layers.Subtract()([gru_features_input, m])\n",
    "        absolute_layer = keras.layers.Lambda(lambda x: keras.backend.abs(x))\n",
    "        attention_features = keras.layers.Concatenate()([ct_mul_q, ct_mul_m, absolute_layer(ct_sub_q), absolute_layer(ct_sub_m)])\n",
    "        attention_dense1 = attention_dense1_layer(attention_features)\n",
    "        attention_dense2 = attention_dense2_layer(attention_dense1)\n",
    "        attention_dense2 = keras.layers.Flatten()(attention_dense2)\n",
    "        attention_score = keras.layers.Activation(\"softmax\")(attention_dense2)\n",
    "        attention_score = keras.layers.Reshape((num_sentences, 1))(attention_score)\n",
    "        gru_feature_input_timestep = keras.layers.Lambda(lambda x: tf.unstack(x, axis=1))(gru_features_input)\n",
    "        attention_timestep = keras.layers.Lambda(lambda x: tf.unstack(x, axis=1))(attention_score)\n",
    "        assert len(gru_feature_input_timestep) == num_sentences\n",
    "        assert len(attention_timestep) == num_sentences\n",
    "        for i in range(num_sentences):\n",
    "            h = m\n",
    "            rt_kernel = rt_kernel_layer(gru_feature_input_timestep[i])\n",
    "            rt_recurrent = rt_recurrent_layer(h)\n",
    "            reset_gate = keras.layers.Add()([rt_kernel, rt_recurrent])\n",
    "            reset_gate = keras.layers.Activation(\"sigmoid\")(reset_gate)\n",
    "            h_bar_kernel = h_bar_kernel_layer(gru_feature_input_timestep[i])\n",
    "            h_bar_recurrent = h_bar_recurrent_layer(h)\n",
    "            h_bar_recurrent = keras.layers.Multiply()([h_bar_recurrent, reset_gate])\n",
    "            h_bar = keras.layers.Add()([h_bar_recurrent, h_bar_kernel])\n",
    "            h_bar = keras.layers.Activation(\"tanh\")(h_bar)\n",
    "            update_attention_gate = keras.layers.RepeatVector(num_cells * 2)(attention_timestep[i])\n",
    "            update_attention_gate = keras.layers.Flatten()(update_attention_gate)\n",
    "            one_minus_attention_gate = keras.layers.Lambda(lambda x: np.ones(num_cells * 2) - x)(update_attention_gate)\n",
    "            h = keras.layers.Add()([keras.layers.Multiply()([update_attention_gate, h_bar]), keras.layers.Multiply()([one_minus_attention_gate, h])])\n",
    "        m = keras.layers.Dense(num_cells * 2, activation=\"relu\")(keras.layers.Concatenate()([m, h, gru_features_question]))\n",
    "    dense_output = keras.layers.Dense(vocab_size, activation=\"softmax\")(keras.layers.Concatenate()([m, gru_features_question]))\n",
    "    model = keras.models.Model(inputs=[input_sentence, question_sentence], outputs=dense_output)\n",
    "    return model\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 29s 32ms/sample - loss: 2.1968 - acc: 0.1578 - val_loss: 1.9475 - val_acc: 0.1800\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 22s 25ms/sample - loss: 1.8189 - acc: 0.1689 - val_loss: 1.8123 - val_acc: 0.1100\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 22s 24ms/sample - loss: 1.8123 - acc: 0.1589 - val_loss: 1.8839 - val_acc: 0.1100\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 1.8162 - acc: 0.1656 - val_loss: 1.8258 - val_acc: 0.2000\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 1.8074 - acc: 0.1844 - val_loss: 1.7827 - val_acc: 0.2300\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 1.8091 - acc: 0.1744 - val_loss: 1.8252 - val_acc: 0.1300\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 1.8002 - acc: 0.1678 - val_loss: 1.7728 - val_acc: 0.2200\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 1.8017 - acc: 0.1633 - val_loss: 1.8648 - val_acc: 0.0700\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 22s 24ms/sample - loss: 1.8042 - acc: 0.1656 - val_loss: 1.7748 - val_acc: 0.1900\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 1.8028 - acc: 0.1867 - val_loss: 1.8207 - val_acc: 0.1900\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 1.8002 - acc: 0.1678 - val_loss: 1.8073 - val_acc: 0.2000\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 22s 24ms/sample - loss: 1.7978 - acc: 0.1867 - val_loss: 1.7835 - val_acc: 0.2300\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 1.8045 - acc: 0.1722 - val_loss: 1.7847 - val_acc: 0.2000\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 1.8045 - acc: 0.1578 - val_loss: 1.8011 - val_acc: 0.2000\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 1.7995 - acc: 0.1833 - val_loss: 1.7973 - val_acc: 0.2000\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 1.8036 - acc: 0.1689 - val_loss: 1.7994 - val_acc: 0.2200\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 1.8021 - acc: 0.1744 - val_loss: 1.8028 - val_acc: 0.1500\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 1.7927 - acc: 0.1811 - val_loss: 1.7895 - val_acc: 0.2500\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 1.7930 - acc: 0.1989 - val_loss: 1.7915 - val_acc: 0.2200\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 1.7915 - acc: 0.1744 - val_loss: 1.8001 - val_acc: 0.1500\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 23s 26ms/sample - loss: 1.8043 - acc: 0.1744 - val_loss: 1.8075 - val_acc: 0.1500\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 23s 25ms/sample - loss: 1.7940 - acc: 0.1878 - val_loss: 1.8038 - val_acc: 0.2000\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 24s 26ms/sample - loss: 1.7946 - acc: 0.1822 - val_loss: 1.7989 - val_acc: 0.2500\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 23s 26ms/sample - loss: 1.7939 - acc: 0.1789 - val_loss: 1.7951 - val_acc: 0.2100\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 28s 31ms/sample - loss: 1.7943 - acc: 0.1856 - val_loss: 1.7962 - val_acc: 0.2200\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 22s 24ms/sample - loss: 1.7959 - acc: 0.1889 - val_loss: 1.7841 - val_acc: 0.2000\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 1.8047 - acc: 0.1556 - val_loss: 1.8045 - val_acc: 0.2600\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 24s 27ms/sample - loss: 1.7919 - acc: 0.1922 - val_loss: 1.7903 - val_acc: 0.2500\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 25s 27ms/sample - loss: 1.7913 - acc: 0.1922 - val_loss: 1.7931 - val_acc: 0.2100\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 23s 25ms/sample - loss: 1.7912 - acc: 0.1889 - val_loss: 1.7907 - val_acc: 0.2100\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 25s 28ms/sample - loss: 1.7915 - acc: 0.1822 - val_loss: 1.7987 - val_acc: 0.2000\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 23s 26ms/sample - loss: 1.7892 - acc: 0.2000 - val_loss: 1.7871 - val_acc: 0.2500\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 22s 25ms/sample - loss: 1.7897 - acc: 0.1733 - val_loss: 1.8095 - val_acc: 0.2000\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 21s 24ms/sample - loss: 1.7840 - acc: 0.1978 - val_loss: 1.7731 - val_acc: 0.2300\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 24s 26ms/sample - loss: 1.7459 - acc: 0.2444 - val_loss: 1.7265 - val_acc: 0.2800\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 25s 27ms/sample - loss: 1.6420 - acc: 0.3222 - val_loss: 1.6279 - val_acc: 0.3500\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 23s 25ms/sample - loss: 1.5894 - acc: 0.3711 - val_loss: 1.5668 - val_acc: 0.4100\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 22s 25ms/sample - loss: 1.4500 - acc: 0.4622 - val_loss: 1.4503 - val_acc: 0.4700\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 24s 27ms/sample - loss: 1.3853 - acc: 0.4989 - val_loss: 1.4679 - val_acc: 0.4500\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 22s 25ms/sample - loss: 1.3023 - acc: 0.5422 - val_loss: 1.3554 - val_acc: 0.4800\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 22s 24ms/sample - loss: 1.2319 - acc: 0.5567 - val_loss: 1.2580 - val_acc: 0.5700\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 24s 27ms/sample - loss: 1.1529 - acc: 0.5744 - val_loss: 1.1591 - val_acc: 0.6200\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 28s 31ms/sample - loss: 1.0953 - acc: 0.6022 - val_loss: 1.1174 - val_acc: 0.6100\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 26s 29ms/sample - loss: 0.9704 - acc: 0.6378 - val_loss: 1.0864 - val_acc: 0.6200\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 0.9204 - acc: 0.6556 - val_loss: 0.9741 - val_acc: 0.6500\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 21s 23ms/sample - loss: 0.9163 - acc: 0.6589 - val_loss: 0.9949 - val_acc: 0.6400\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 23s 25ms/sample - loss: 0.8773 - acc: 0.6756 - val_loss: 0.9853 - val_acc: 0.6400\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 52s 57ms/sample - loss: 0.8305 - acc: 0.6789 - val_loss: 1.0656 - val_acc: 0.6100\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 52s 58ms/sample - loss: 0.8125 - acc: 0.6944 - val_loss: 1.0165 - val_acc: 0.6800\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 51s 57ms/sample - loss: 0.7856 - acc: 0.6933 - val_loss: 1.0204 - val_acc: 0.7000\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 52s 57ms/sample - loss: 0.7594 - acc: 0.7011 - val_loss: 0.9396 - val_acc: 0.7000\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 49s 55ms/sample - loss: 0.7305 - acc: 0.7200 - val_loss: 1.0116 - val_acc: 0.6900\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 51s 57ms/sample - loss: 0.7661 - acc: 0.7189 - val_loss: 1.0695 - val_acc: 0.6200\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.7318 - acc: 0.7178 - val_loss: 1.0995 - val_acc: 0.6900\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.7024 - acc: 0.7489 - val_loss: 1.1902 - val_acc: 0.6900\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.6966 - acc: 0.7422 - val_loss: 0.9925 - val_acc: 0.7100\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.6645 - acc: 0.7656 - val_loss: 1.0033 - val_acc: 0.7200\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 51s 57ms/sample - loss: 0.6380 - acc: 0.7633 - val_loss: 0.9451 - val_acc: 0.7400\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.6411 - acc: 0.7733 - val_loss: 1.1173 - val_acc: 0.6900\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/900 [==============================] - 50s 56ms/sample - loss: 0.6377 - acc: 0.7711 - val_loss: 0.9579 - val_acc: 0.7200\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 53s 59ms/sample - loss: 0.6242 - acc: 0.7767 - val_loss: 1.1303 - val_acc: 0.7000\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.6155 - acc: 0.7856 - val_loss: 1.1754 - val_acc: 0.7400\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 50s 56ms/sample - loss: 0.5791 - acc: 0.8033 - val_loss: 1.1356 - val_acc: 0.7100\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 50s 56ms/sample - loss: 0.5612 - acc: 0.8022 - val_loss: 1.1204 - val_acc: 0.7100\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 51s 57ms/sample - loss: 0.5386 - acc: 0.8222 - val_loss: 0.9430 - val_acc: 0.7100\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 50s 56ms/sample - loss: 0.5290 - acc: 0.8244 - val_loss: 1.0848 - val_acc: 0.6900\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 52s 57ms/sample - loss: 0.5159 - acc: 0.8167 - val_loss: 1.0767 - val_acc: 0.7100\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 49s 55ms/sample - loss: 0.4854 - acc: 0.8367 - val_loss: 1.1955 - val_acc: 0.7000\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 51s 57ms/sample - loss: 0.4572 - acc: 0.8467 - val_loss: 1.1581 - val_acc: 0.7200\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 49s 55ms/sample - loss: 0.4724 - acc: 0.8444 - val_loss: 1.3994 - val_acc: 0.6800\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 52s 57ms/sample - loss: 0.4870 - acc: 0.8422 - val_loss: 1.2395 - val_acc: 0.6800\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.4869 - acc: 0.8389 - val_loss: 1.2967 - val_acc: 0.6700\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 52s 57ms/sample - loss: 0.4290 - acc: 0.8611 - val_loss: 1.2871 - val_acc: 0.6700\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 51s 57ms/sample - loss: 0.3625 - acc: 0.8778 - val_loss: 1.5238 - val_acc: 0.6600\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.3375 - acc: 0.8856 - val_loss: 1.3964 - val_acc: 0.6800\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.3258 - acc: 0.8911 - val_loss: 1.2361 - val_acc: 0.6600\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.2812 - acc: 0.9144 - val_loss: 1.4357 - val_acc: 0.6400\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.2757 - acc: 0.9122 - val_loss: 1.6845 - val_acc: 0.6300\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.2540 - acc: 0.9133 - val_loss: 1.4654 - val_acc: 0.6700\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.2130 - acc: 0.9367 - val_loss: 1.4436 - val_acc: 0.6600\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.2401 - acc: 0.9400 - val_loss: 1.6639 - val_acc: 0.6500\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 51s 57ms/sample - loss: 0.2380 - acc: 0.9267 - val_loss: 1.4681 - val_acc: 0.6700\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.1707 - acc: 0.9533 - val_loss: 1.6014 - val_acc: 0.7100\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 50s 56ms/sample - loss: 0.1467 - acc: 0.9567 - val_loss: 1.7119 - val_acc: 0.6200\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.1270 - acc: 0.9633 - val_loss: 1.6382 - val_acc: 0.6900\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 52s 58ms/sample - loss: 0.1053 - acc: 0.9733 - val_loss: 1.9417 - val_acc: 0.6500\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 52s 57ms/sample - loss: 0.0930 - acc: 0.9778 - val_loss: 1.8000 - val_acc: 0.6700\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 50s 56ms/sample - loss: 0.0745 - acc: 0.9789 - val_loss: 1.7268 - val_acc: 0.7000\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.0683 - acc: 0.9800 - val_loss: 1.7281 - val_acc: 0.6600\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 50s 56ms/sample - loss: 0.0593 - acc: 0.9878 - val_loss: 1.7444 - val_acc: 0.6600\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.0436 - acc: 0.9911 - val_loss: 1.9924 - val_acc: 0.6900\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.0402 - acc: 0.9922 - val_loss: 2.2196 - val_acc: 0.6500\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 50s 56ms/sample - loss: 0.0502 - acc: 0.9878 - val_loss: 1.9967 - val_acc: 0.6700\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.0472 - acc: 0.9867 - val_loss: 2.0045 - val_acc: 0.6600\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.0637 - acc: 0.9811 - val_loss: 2.4273 - val_acc: 0.6700\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 49s 55ms/sample - loss: 0.0608 - acc: 0.9811 - val_loss: 2.0007 - val_acc: 0.6600\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 50s 55ms/sample - loss: 0.0732 - acc: 0.9778 - val_loss: 2.0919 - val_acc: 0.6500\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 51s 56ms/sample - loss: 0.1021 - acc: 0.9756 - val_loss: 1.9255 - val_acc: 0.7100\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 52s 58ms/sample - loss: 0.0912 - acc: 0.9744 - val_loss: 2.5142 - val_acc: 0.6100\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 52s 57ms/sample - loss: 0.0648 - acc: 0.9789 - val_loss: 2.3007 - val_acc: 0.7200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xc582cfbcc0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(maxlen, len(tokenizer.word_index), embedding_size, num_episodes, num_sentences, num_cells)\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\"model.{epoch:02d}-{val_loss:.2f}.hdf5\", save_best_only=True)\n",
    "model.fit([input_sentences, questions], answers, epochs=100, batch_size=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
