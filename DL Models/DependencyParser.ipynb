{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"C:/Users/Himanshu/Downloads/a3/a3/data/dev.gold.conll\"\n",
    "num_cells = 128\n",
    "num_epochs = 50\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess:\n",
    "    def __init__(self):\n",
    "        self.transitions = {\"left\": 0, \"right\": 1, \"shift\": 2}\n",
    "        self.model = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_dataset(filename):\n",
    "        examples = []\n",
    "        with open(filename, \"r+\") as f:\n",
    "            words, pos, head, label = [], [], [], []\n",
    "            for line in f.readlines():\n",
    "                line_split = line.strip().split(\"\\t\")\n",
    "                if len(line_split) == 10:\n",
    "                    words.append(line_split[1])\n",
    "                    pos.append(line_split[3])\n",
    "                    head.append(int(line_split[6]))\n",
    "                    label.append(line_split[7])\n",
    "                else:\n",
    "                    examples.append([words, pos, head, label])\n",
    "                    words, pos, head, label = [], [], [], []\n",
    "        return examples\n",
    "    \n",
    "    def create_tok2id(self, examples):\n",
    "        tok2id = {}\n",
    "        tok2id[\"UNK\"] = 0\n",
    "        tok2id[\"ROOT\"] = 1\n",
    "        tok2id[\"ROOTPOS\"] = 2\n",
    "        tok2id[\"ROOTHEAD\"] = 3\n",
    "        tok2id[\"ROOTLABEL\"] = 4\n",
    "        tok2id[\"PAD\"] = 5\n",
    "        tok2id[\"PPAD\"] = 6\n",
    "        tok2id[\"LPAD\"] = 7\n",
    "        ind = len(tok2id)\n",
    "        for example in examples:\n",
    "            for word in example[0]:\n",
    "                if word not in tok2id:\n",
    "                    tok2id[word] = ind\n",
    "                    ind += 1\n",
    "            for pos in example[1]:\n",
    "                if pos not in tok2id:\n",
    "                    tok2id[pos] = ind\n",
    "                    ind += 1\n",
    "            for label in example[3]:\n",
    "                if label not in tok2id:\n",
    "                    tok2id[label] = ind\n",
    "                    ind += 1\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = { tok_id: tok for tok, tok_id in self.tok2id.items() }\n",
    "    \n",
    "    def get_tok2id(self, tok):\n",
    "        if tok in self.tok2id:\n",
    "            return self.tok2id[tok]\n",
    "        else:\n",
    "            return self.tok2id[\"UNK\"]\n",
    "    \n",
    "    def get_oracle(self, stack, buf, example):\n",
    "        if len(stack) < 2:\n",
    "            return self.transitions[\"shift\"]\n",
    "        \n",
    "        i1 = stack[-1]\n",
    "        i2 = stack[-2]\n",
    "        h1 = example[2][i1]\n",
    "        h2 = example[2][i2]\n",
    "        if i2 > 0 and h2 == i1 and all(False if example[2][b] == i2 else True for b in buf):\n",
    "            return self.transitions[\"left\"]\n",
    "        elif i2 >=0 and h1 == i2 and all(False if example[2][b] == i1 else True for b in buf):\n",
    "            return self.transitions[\"right\"]\n",
    "        else:\n",
    "            return self.transitions[\"shift\"]\n",
    "    \n",
    "    def get_features(self, stack, buffer, arcs, example):\n",
    "        i1 = stack[-1]\n",
    "        i2 = stack[-2] if len(stack) >= 2 else -1\n",
    "        w1 = self.get_tok2id(example[0][i1])\n",
    "        w2 = self.get_tok2id(example[0][i2]) if i2 != -1 else self.get_tok2id(\"PAD\")\n",
    "        wb1 = self.get_tok2id(example[0][buffer[0]]) if len(buffer) > 0 else self.get_tok2id(\"PAD\")\n",
    "        lci1 = [arc[1] for arc in arcs if arc[0] == i1 and arc[1] < i1]\n",
    "        rci1 = [arc[1] for arc in arcs if arc[0] == i1 and arc[1] > i1]\n",
    "        lci2 = [arc[1] for arc in arcs if arc[0] == i2 and arc[1] < i2]\n",
    "        rci2 = [arc[1] for arc in arcs if arc[0] == i2 and arc[1] > i2]\n",
    "        wlc1 = self.get_tok2id(example[0][lci1[0]]) if len(lci1) > 0 else self.get_tok2id(\"PAD\")\n",
    "        wrc1 = self.get_tok2id(example[0][rci1[0]]) if len(rci1) > 0 else self.get_tok2id(\"PAD\")\n",
    "        wlc2 = self.get_tok2id(example[0][lci2[0]]) if len(lci2) > 0 else self.get_tok2id(\"PAD\")\n",
    "        wrc2 = self.get_tok2id(example[0][rci2[0]]) if len(rci2) > 0 else self.get_tok2id(\"PAD\")\n",
    "        p1 = self.get_tok2id(example[1][i1])\n",
    "        p2 = self.get_tok2id(example[1][i2]) if i2 != -1 else self.get_tok2id(\"PPAD\")\n",
    "        pb1 = self.get_tok2id(example[1][buffer[0]]) if len(buffer) > 0 else self.get_tok2id(\"PPAD\")\n",
    "        plc1 = self.get_tok2id(example[1][lci1[0]]) if len(lci1) > 0 else self.get_tok2id(\"PAD\")\n",
    "        prc1 = self.get_tok2id(example[1][rci1[0]]) if len(rci1) > 0 else self.get_tok2id(\"PAD\")\n",
    "        plc2 = self.get_tok2id(example[1][lci2[0]]) if len(lci2) > 0 else self.get_tok2id(\"PAD\")\n",
    "        prc2 = self.get_tok2id(example[1][rci2[0]]) if len(rci2) > 0 else self.get_tok2id(\"PAD\")\n",
    "        return [w1, w2, wb1, wlc1, wrc1, wlc2, wrc2, p1, p2, pb1, plc1, prc1, plc2, prc2]\n",
    "    \n",
    "    def parse_example(self, given_example):\n",
    "        stack = [0]\n",
    "        buffer = list(range(1, len(given_example[0]) + 1))\n",
    "        arcs = []\n",
    "        example = [[], [], []]\n",
    "        example[0] = [\"ROOT\"] + given_example[0]\n",
    "        example[1] = [\"ROOTPOS\"] + given_example[1]\n",
    "        example[2] = [\"ROOTHEAD\"] + given_example[2]\n",
    "        input_features = []\n",
    "        outputs = []\n",
    "        while not (len(buffer) == 0 and stack == [0]):\n",
    "            gold_parse = self.get_oracle(stack, buffer, example)\n",
    "            features = self.get_features(stack, buffer, arcs, example)\n",
    "            input_features.append(features)\n",
    "            outputs.append(gold_parse)\n",
    "            if gold_parse == 0:\n",
    "                arcs.append([stack[-1], stack[-2], gold_parse])\n",
    "                stack = stack[:-2] + [stack[-1]]\n",
    "            elif gold_parse == 1:\n",
    "                arcs.append([stack[-2], stack[-1], gold_parse])\n",
    "                stack = stack[:-1]\n",
    "            else:\n",
    "                stack = stack + [buffer[0]]\n",
    "                buffer = buffer[1: ]\n",
    "        return input_features, outputs\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, inps):\n",
    "        inps = inps[:]\n",
    "        assert self.model\n",
    "        stacks = []\n",
    "        buffers = []\n",
    "        arcs = []\n",
    "        outputs = [0] * len(inps)\n",
    "        for i in range(len(inps)):\n",
    "            batch = []\n",
    "            stack = [0]\n",
    "            buffer = list(range(1, len(inps[i][0]) + 1))\n",
    "            arc = []\n",
    "            stacks.append(stack)\n",
    "            buffers.append(buffer)\n",
    "            arcs.append(arc)\n",
    "            inps[i][0] = [\"ROOT\"] + inps[i][0]\n",
    "            inps[i][1] = [\"ROOTPOS\"] + inps[i][1]\n",
    "        example_indices = list(range(len(inps)))\n",
    "        while example_indices:\n",
    "            input_features = []\n",
    "            for i in example_indices:\n",
    "                features = self.get_features(stacks[i], buffers[i], arcs[i], inps[i])\n",
    "                input_features.append(features)\n",
    "            predicted_parse = self.model.predict(np.array(input_features))\n",
    "            predicted_parse = np.argmax(predicted_parse, axis=1)\n",
    "            print(predicted_parse)\n",
    "            remaining_indices = []\n",
    "            for index, i in enumerate(example_indices):\n",
    "                gold_parse = predicted_parse[index]\n",
    "                if len(buffers[i]) == 0 and len(stacks[i]) == 1:\n",
    "                    print(\"discarding input {0}\".format(i))\n",
    "                    outputs[i] = arcs[i]\n",
    "                    continue\n",
    "                if gold_parse == 0:\n",
    "                    arcs[i].append([stacks[i][-1], stacks[i][-2], gold_parse])\n",
    "                    stacks[i] = stacks[i][:-2] + [stacks[i][-1]]\n",
    "                    remaining_indices.append(i)\n",
    "                elif gold_parse == 1:\n",
    "                    arcs[i].append([stacks[i][-2], stacks[i][-1], gold_parse])\n",
    "                    stacks[i] = stacks[i][:-1]\n",
    "                    remaining_indices.append(i)\n",
    "                else:\n",
    "                    if len(buffers[i]) != 0:\n",
    "                        stacks[i] = stacks[i] + [buffers[i][0]]\n",
    "                        buffers[i] = buffers[i][1: ]\n",
    "                        remaining_indices.append(i)\n",
    "                    else:\n",
    "                        print(\"in ELSE\")\n",
    "                        outputs[i] = arcs[i]\n",
    "            example_indices = remaining_indices\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "352\n",
      "708\n",
      "1316\n",
      "1561\n",
      "(79920, 14)\n",
      "(79920,)\n"
     ]
    }
   ],
   "source": [
    "data_process = DataProcess()\n",
    "examples = data_process.read_dataset(data_location)\n",
    "data_process.create_tok2id(examples)\n",
    "model_features, model_outputs = [], []\n",
    "for ind, example in enumerate(examples):\n",
    "    try:\n",
    "        features, outputs = data_process.parse_example(example)\n",
    "        model_features += features\n",
    "        model_outputs += outputs\n",
    "    except:\n",
    "        print(ind)\n",
    "model_features = np.array(model_features)\n",
    "model_outputs = np.array(model_outputs)\n",
    "print(model_features.shape)\n",
    "print(model_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(model_outputs, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 71928 samples, validate on 7992 samples\n",
      "Epoch 1/3\n",
      "71928/71928 [==============================] - 10s 144us/sample - loss: 0.8129 - acc: 0.6452 - val_loss: 0.5484 - val_acc: 0.7974\n",
      "Epoch 2/3\n",
      "71928/71928 [==============================] - 8s 115us/sample - loss: 0.4345 - acc: 0.8382 - val_loss: 0.3477 - val_acc: 0.8705\n",
      "Epoch 3/3\n",
      "71928/71928 [==============================] - 8s 116us/sample - loss: 0.3191 - acc: 0.8801 - val_loss: 0.2914 - val_acc: 0.8884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xa183e4b588>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inp = tf.keras.layers.Input(shape=(14, ))\n",
    "embeddings = tf.keras.layers.Embedding(input_dim=len(data_process.tok2id), output_dim=100)(model_inp)\n",
    "flat_emb = tf.keras.layers.Flatten()(embeddings)\n",
    "flat_emb = tf.keras.layers.Dropout(0.2)(flat_emb)\n",
    "dense1 = tf.keras.layers.Dense(num_cells, activation=\"elu\")(flat_emb)\n",
    "dense1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
    "dense2 = tf.keras.layers.Dense(num_cells, activation=\"elu\")(dense1)\n",
    "dense2 = tf.keras.layers.Dropout(0.2)(dense2)\n",
    "model_out = tf.keras.layers.Dense(3, activation=\"softmax\")(dense2)\n",
    "model = tf.keras.models.Model(inputs=model_inp, outputs=model_out)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./dpmodels/model.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "model.fit(model_features, model_outputs, epochs=3,\n",
    "          batch_size=batch_size, validation_split=0.1, callbacks=[model_checkpoint, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2]\n",
      "[2 2 2]\n",
      "[0 0 0]\n",
      "[2 2 2]\n",
      "[2 1 0]\n",
      "[2 2 2]\n",
      "[2 2 2]\n",
      "[0 0 0]\n",
      "[0 2 2]\n",
      "[0 2 2]\n",
      "[2 2 0]\n",
      "[1 2 1]\n",
      "[2 0 2]\n",
      "[2 0 2]\n",
      "[0 2 2]\n",
      "[1 2 0]\n",
      "[2 0 0]\n",
      "[2 1 2]\n",
      "[0 1 1]\n",
      "[0 1 1]\n",
      "[0 1 2]\n",
      "[2 2 1]\n",
      "[2 1 2]\n",
      "[2 2 2]\n",
      "[0 1 2]\n",
      "[0 2 0]\n",
      "[2 1 2]\n",
      "[2 2 0]\n",
      "[2 1 0]\n",
      "[2 2 2]\n",
      "[2 2 2]\n",
      "[2 0 2]\n",
      "[0 0 0]\n",
      "[0 2 0]\n",
      "[0 2 1]\n",
      "[0 2 1]\n",
      "[0 2 1]\n",
      "[1 0 2]\n",
      "[2 0 1]\n",
      "[2 0 1]\n",
      "[0 1 1]\n",
      "discarding input 2\n",
      "[2 2]\n",
      "[1 2]\n",
      "[1 2]\n",
      "[2 2]\n",
      "[1 2]\n",
      "[2 0]\n",
      "[2 0]\n",
      "[2 0]\n",
      "[2 0]\n",
      "[0 1]\n",
      "[0 2]\n",
      "[0 2]\n",
      "[2 0]\n",
      "[2 2]\n",
      "[2 2]\n",
      "[0 2]\n",
      "[0 0]\n",
      "[2 0]\n",
      "[1 2]\n",
      "[2 1]\n",
      "[0 2]\n",
      "[2 2]\n",
      "[2 2]\n",
      "[2 0]\n",
      "[0 0]\n",
      "[0 2]\n",
      "[1 2]\n",
      "[1 2]\n",
      "[1 2]\n",
      "[1 2]\n",
      "[2 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "discarding input 0\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[2]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "discarding input 1\n",
      "ROOT Influential members of the House Ways and Means Committee introduced legislation that would restrict how the new savings-and-loan bailout agency can raise capital , creating another potential obstacle to the government 's sale of sick thrifts .\n",
      "members <-- Influential\n",
      "Ways <-- House\n",
      "Ways <-- the\n",
      "Ways <-- of\n",
      "Ways <-- and\n",
      "Committee <-- Means\n",
      "Ways <-- Committee\n",
      "legislation <-- introduced\n",
      "legislation <-- Ways\n",
      "legislation <-- members\n",
      "restrict <-- would\n",
      "restrict <-- that\n",
      "agency <-- bailout\n",
      "agency <-- savings-and-loan\n",
      "agency <-- new\n",
      "agency <-- the\n",
      "agency <-- how\n",
      "restrict <-- agency\n",
      "raise <-- can\n",
      "raise <-- capital\n",
      "restrict <-- raise\n",
      "restrict <-- ,\n",
      "obstacle <-- potential\n",
      "obstacle <-- another\n",
      "obstacle <-- creating\n",
      "government <-- the\n",
      "government <-- to\n",
      "government <-- 's\n",
      "sale <-- government\n",
      "thrifts <-- sick\n",
      "thrifts <-- of\n",
      "sale <-- thrifts\n",
      "obstacle <-- sale\n",
      "restrict <-- obstacle\n",
      "legislation <-- restrict\n",
      "legislation <-- .\n",
      "ROOT <-- legislation\n",
      "ROOT The bill , whose backers include Chairman Dan Rostenkowski -LRB- D. , Ill. -RRB- , would prevent the Resolution Trust Corp. from raising temporary working capital by having an RTC-owned bank or thrift issue debt that would n't be counted on the federal budget .\n",
      "bill <-- The\n",
      "bill <-- ,\n",
      "backers <-- whose\n",
      "Rostenkowski <-- Dan\n",
      "Rostenkowski <-- Chairman\n",
      "D. <-- -LRB-\n",
      "Rostenkowski <-- D.\n",
      "include <-- Rostenkowski\n",
      "backers <-- include\n",
      "bill <-- backers\n",
      "bill <-- ,\n",
      "bill <-- Ill.\n",
      "bill <-- -RRB-\n",
      "bill <-- ,\n",
      "prevent <-- would\n",
      "prevent <-- bill\n",
      "Corp. <-- Trust\n",
      "Corp. <-- Resolution\n",
      "Corp. <-- the\n",
      "prevent <-- Corp.\n",
      "capital <-- working\n",
      "capital <-- temporary\n",
      "capital <-- raising\n",
      "capital <-- from\n",
      "prevent <-- capital\n",
      "having <-- by\n",
      "bank <-- RTC-owned\n",
      "bank <-- an\n",
      "bank <-- or\n",
      "debt <-- issue\n",
      "debt <-- thrift\n",
      "counted <-- be\n",
      "counted <-- n't\n",
      "counted <-- would\n",
      "counted <-- that\n",
      "counted <-- debt\n",
      "counted <-- bank\n",
      "counted <-- having\n",
      "budget <-- federal\n",
      "budget <-- the\n",
      "budget <-- on\n",
      "counted <-- budget\n",
      "prevent <-- counted\n",
      "prevent <-- .\n",
      "ROOT <-- prevent\n",
      "ROOT The bill intends to restrict the RTC to Treasury borrowings only , unless the agency receives specific congressional authorization .\n",
      "bill <-- The\n",
      "intends <-- bill\n",
      "restrict <-- to\n",
      "RTC <-- the\n",
      "restrict <-- RTC\n",
      "borrowings <-- Treasury\n",
      "borrowings <-- to\n",
      "borrowings <-- only\n",
      "restrict <-- borrowings\n",
      "restrict <-- ,\n",
      "agency <-- the\n",
      "receives <-- agency\n",
      "receives <-- unless\n",
      "authorization <-- congressional\n",
      "authorization <-- specific\n",
      "receives <-- authorization\n",
      "restrict <-- receives\n",
      "intends <-- restrict\n",
      "intends <-- .\n",
      "ROOT <-- intends\n"
     ]
    }
   ],
   "source": [
    "data_process.set_model(model)\n",
    "out = data_process.predict(examples[:3])\n",
    "for i in range(len(examples[:3])):\n",
    "    print(\" \".join(examples[i][0]))\n",
    "    for each in out[i]:\n",
    "        print(examples[i][0][each[0]], \"<--\", examples[i][0][each[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
