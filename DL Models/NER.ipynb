{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Himanshu\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"E:/datasets/ner/ner_dataset.csv\"\n",
    "window_size = 7\n",
    "num_cells = 64\n",
    "input_length = (window_size * 2) + 1\n",
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_location, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "inputs = []\n",
    "ninputs = []\n",
    "noutputs = []\n",
    "for i in range(len(data)):\n",
    "    if data[i][3] != \"O\":\n",
    "        outputs.append(data[i][3])\n",
    "        start_index = i - window_size\n",
    "        end_index = i + window_size + 1\n",
    "        if start_index < 0:\n",
    "            start_index = 0\n",
    "        if end_index > len(data) - 1:\n",
    "            end_index = len(data) - 1\n",
    "        input_window = []\n",
    "        for j in range(start_index, i + 1):\n",
    "            if type(data[j][0]) is float:\n",
    "                input_window.append(data[j][1])\n",
    "            else:\n",
    "                input_window = [\"UNK\"] * len(input_window) + [data[j][1]]\n",
    "        for j in range(i+1, end_index):\n",
    "            if type(data[j][0]) is float:\n",
    "                input_window.append(data[j][1])\n",
    "            else:\n",
    "                input_window = input_window + ((window_size * 2 + 1) - len(input_window)) * [\"UNK\"]\n",
    "                break\n",
    "        inputs.append(input_window)\n",
    "    else:\n",
    "        noutputs.append(data[i][3])\n",
    "        start_index = i - window_size\n",
    "        end_index = i + window_size + 1\n",
    "        if start_index < 0:\n",
    "            start_index = 0\n",
    "        if end_index > len(data) - 1:\n",
    "            end_index = len(data) - 1\n",
    "        input_window = []\n",
    "        for j in range(start_index, i + 1):\n",
    "            if type(data[j][0]) is float:\n",
    "                input_window.append(data[j][1])\n",
    "            else:\n",
    "                input_window = [\"UNK\"] * len(input_window) + [data[j][1]]\n",
    "        for j in range(i+1, end_index):\n",
    "            if type(data[j][0]) is float:\n",
    "                input_window.append(data[j][1])\n",
    "            else:\n",
    "                input_window = input_window + ((window_size * 2 + 1) - len(input_window)) * [\"UNK\"]\n",
    "                break\n",
    "        ninputs.append(input_window)\n",
    "outputs = np.array(outputs)\n",
    "noutputs = np.array(noutputs)\n",
    "inputs = np.array([\" \".join(i) for i in inputs if len(i) == input_length])\n",
    "ninputs = np.array([\" \".join(i) for i in ninputs if len(i) == input_length])\n",
    "nsample = np.arange(len(ninputs))\n",
    "np.random.shuffle(nsample)\n",
    "nsample = nsample[: len(inputs)]\n",
    "ninputs = ninputs[nsample]\n",
    "noutputs = noutputs[nsample]\n",
    "inputs = np.concatenate([inputs, ninputs])\n",
    "outputs = np.concatenate([outputs, noutputs])\n",
    "sample = np.arange(len(inputs))\n",
    "np.random.shuffle(sample)\n",
    "inputs = inputs[sample]\n",
    "outputs = outputs[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(inputs)\n",
    "model_inputs = tokenizer.texts_to_sequences(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_outputs = np.unique(outputs)\n",
    "output_mapping = {j:i for i, j in enumerate(unique_outputs)}\n",
    "outputs = [output_mapping[output] for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPModel:\n",
    "    def __init__(self, num_cells, embedding_size, vocab_size, input_length, output_size):\n",
    "        self.num_cells = num_cells\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_layer = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_size))\n",
    "        self.output_size = output_size\n",
    "        self.input_length = input_length\n",
    "        self.W1 = np.random.uniform(-0.05, 0.05, (input_length * embedding_size, num_cells))\n",
    "        self.B1 = np.zeros((num_cells))\n",
    "        self.W2 = np.random.uniform(-0.05, 0.05, (num_cells, output_size))\n",
    "        self.B2 = np.zeros((output_size))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        for i in range(len(x)):\n",
    "            if x[i] < 0:\n",
    "                x[i] = 0\n",
    "        return x\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x = np.exp(x - np.max(x))\n",
    "        x = x/np.sum(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        self.emb_out = []\n",
    "        for i in input_data:\n",
    "            self.emb_out.extend(self.embedding_layer[i])\n",
    "        self.W1_dot_emb_out = np.dot(self.emb_out, self.W1)\n",
    "        self.W1_dot_emb_out_bias = self.W1_dot_emb_out + self.B1\n",
    "        self.h1 = self.relu(self.W1_dot_emb_out_bias)\n",
    "        self.W2_dot_h1 = np.dot(self.h1, self.W2)\n",
    "        self.W2_dot_h1_bias = self.W2_dot_h1 + self.B2\n",
    "        self.out = self.softmax(self.W2_dot_h1_bias)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, output_data):\n",
    "        self.delta_out = self.out - output_data\n",
    "        self.delta_W2 = np.outer(self.h1, self.delta_out)\n",
    "        self.delta_B2 = np.dot(self.delta_out, np.identity(self.output_size))\n",
    "        temp = np.dot(self.delta_out, self.W2.transpose())\n",
    "        self.delta_h1 = []\n",
    "        for i in range(len(temp)):\n",
    "            if (self.W1_dot_emb_out_bias[i] > 0):\n",
    "                self.delta_h1.append(temp[i])\n",
    "            else:\n",
    "                self.delta_h1.append(0)\n",
    "        self.delta_W1 = np.outer(self.emb_out, self.delta_h1)\n",
    "        self.delta_B1 = np.dot(self.delta_h1, np.identity(self.num_cells))\n",
    "        self.delta_emb_out = np.dot(self.delta_h1, self.W1.transpose())\n",
    "    \n",
    "    def apply_grad(self, lr):\n",
    "        self.W2 -= lr * self.delta_W2\n",
    "        self.B2 -= lr * self.delta_B2\n",
    "        self.W1 -= lr * self.delta_W1\n",
    "        self.B1 -= lr * self.delta_B1\n",
    "        self.embedding_layer[self.input_data] -= lr * self.delta_emb_out.reshape(self.input_length, self.embedding_size)\n",
    "    \n",
    "    def fit(self, inputs, outputs, epochs, lr):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0.0\n",
    "            t = tqdm(range(len(inputs)))\n",
    "            for i in t:\n",
    "                model_output = np.zeros((self.output_size))\n",
    "                model_output[outputs[i]] = 1\n",
    "                self.forward(inputs[i])\n",
    "                self.backward(model_output)\n",
    "                self.apply_grad(lr)\n",
    "                loss += -1 * (np.log(np.sum(model_output * self.out)))\n",
    "                t.set_postfix({\"loss\": loss/(i + 1), \"epoch\": str(epoch + 1)})\n",
    "    \n",
    "    def predict(self, input_sentence, tokenizer, window_size, output_mapping):\n",
    "        input_sentence = \"UNK \" * window_size + input_sentence + \" UNK\" * window_size\n",
    "        rev_output_mapping = {j: i for i, j in output_mapping.items()}\n",
    "        sequences = tokenizer.texts_to_sequences([input_sentence])[0]\n",
    "        rev_word_index = {j: i for i, j in tokenizer.word_index.items()}\n",
    "        for i in range(window_size, len(sequences) - window_size):\n",
    "            model_input = sequences[i - window_size: i + window_size + 1]\n",
    "            out = self.forward(model_input)\n",
    "            out_class = rev_output_mapping[np.argmax(out)]\n",
    "            print(rev_word_index[sequences[i]], out_class)\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NLPModel(num_cells, embedding_size, len(tokenizer.word_index) + 1, input_length, len(output_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [28:15<00:00, 189.52it/s, loss=1.43, epoch=1]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [28:31<00:00, 187.71it/s, loss=1.03, epoch=2]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [27:06<00:00, 197.57it/s, loss=0.902, epoch=3]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [27:32<00:00, 194.50it/s, loss=0.805, epoch=4]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [27:25<00:00, 195.25it/s, loss=0.742, epoch=5]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [27:51<00:00, 192.19it/s, loss=0.696, epoch=6]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [28:04<00:00, 190.72it/s, loss=0.659, epoch=7]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [28:14<00:00, 189.58it/s, loss=0.625, epoch=8]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [28:10<00:00, 190.10it/s, loss=0.594, epoch=9]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 321330/321330 [28:09<00:00, 190.23it/s, loss=0.562, epoch=10]\n"
     ]
    }
   ],
   "source": [
    "model.fit(model_inputs, outputs, 10, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "international B-geo\n",
      "atomic B-org\n",
      "agency I-org\n",
      "is O\n",
      "to O\n",
      "hold O\n",
      "second I-org\n",
      "day O\n",
      "of O\n",
      "talks O\n",
      "in O\n",
      "vienna I-tim\n",
      "on O\n",
      "wednesday B-geo\n",
      "on O\n",
      "how O\n",
      "to O\n",
      "respond O\n",
      "to O\n",
      "iran B-tim\n",
      "'s O\n",
      "resumption B-geo\n",
      "of O\n",
      "low-level O\n",
      "uranium O\n",
      "conversion O\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"International atomic agency is to hold second day of talks in vienna on wednesday on how to respond to iran 's resumption of low-level uranium conversion .\"\n",
    "model.predict(input_sentence, tokenizer, window_size, output_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
